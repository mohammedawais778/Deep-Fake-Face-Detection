import os
import psutil
import logging
import signal
import sys
from datetime import datetime
import threading
from typing import Dict, List, Optional, Tuple
import time
from functools import wraps
from werkzeug.utils import secure_filename
from flask import Flask, request, jsonify, send_from_directory, render_template, abort
import shutil
import cv2
from PIL import Image

# Import required modules
import torch
from convert_to_torch import DeepfakeDetector
TORCH_AVAILABLE = True

# Configure logging with more detailed format
logging.basicConfig(
    level=logging.DEBUG,  # Change to DEBUG level
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(pathname)s:%(lineno)d',
    handlers=[
        logging.FileHandler('deepfake_detector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MemoryMonitor:
    def __init__(self, threshold_mb=1000):  # 1GB threshold
        self.threshold_mb = threshold_mb
        self.lock = threading.Lock()
        self.monitoring = False
        
    def get_memory_usage(self) -> Dict[str, float]:
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            'rss_mb': memory_info.rss / (1024 * 1024),  # RSS in MB
            'vms_mb': memory_info.vms / (1024 * 1024),  # VMS in MB
            'percent': process.memory_percent()
        }
        
    def start_monitoring(self):
        if not self.monitoring:
            self.monitoring = True
            threading.Thread(target=self._monitor_memory, daemon=True).start()
            
    def _monitor_memory(self):
        while self.monitoring:
            usage = self.get_memory_usage()
            if usage['rss_mb'] > self.threshold_mb:
                logger.warning(f"High memory usage: {usage['rss_mb']:.2f}MB RSS")
                if hasattr(model_manager, 'model'):
                    model_manager.unload_model()  # Free up memory
            time.sleep(60)  # Check every minute
            
memory_monitor = MemoryMonitor()

# Rate limiting
class RateLimiter:
    def __init__(self, max_requests=10, window_seconds=60):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests = {}
        self.lock = threading.Lock()

    def is_allowed(self, key):
        now = time.time()
        with self.lock:
            # Clean old entries
            self.requests = {k: v for k, v in self.requests.items() 
                           if now - v[-1] < self.window_seconds}
            
            if key not in self.requests:
                self.requests[key] = []
            
            self.requests[key] = [t for t in self.requests[key] 
                                if now - t < self.window_seconds]
            
            if len(self.requests[key]) >= self.max_requests:
                return False
            
            self.requests[key].append(now)
            return True

rate_limiter = RateLimiter()

def rate_limit(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not rate_limiter.is_allowed(request.remote_addr):
            abort(429, description="Too many requests")
        return f(*args, **kwargs)
    return decorated_function

# Model management with caching and versioning
class ModelManager:
    def __init__(self):
        self.model = None
        self.model_hash = None
        self.last_load_time = None
        self.lock = threading.Lock()
        self.last_used = time.time()
        self.load_timeout = 30  # seconds
        logger.info("ModelManager initialized")

    def get_model_hash(self, model_path):
        """Get model file hash for version checking"""
        import hashlib
        if os.path.exists(model_path):
            with open(model_path, "rb") as f:
                return hashlib.md5(f.read()).hexdigest()
        return None

    def unload_model(self):
        """Unload model to free up memory"""
        with self.lock:
            if self.model is not None:
                logger.info("Unloading model to free memory")
                if hasattr(self.model, 'cpu'):
                    self.model.cpu()
                self.model = None
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

    def load_model(self, force=False):
        """Thread-safe model loading with caching and timeout"""
        logger.info("Entering load_model function")
        start_time = time.time()
        
        def check_timeout():
            if time.time() - start_time > self.load_timeout:
                raise TimeoutError("Model loading timed out")
        
        with self.lock:
            try:
                model_path = os.path.join("model", "deepfake_detector.pth")
                logger.info(f"Checking model at path: {model_path}")
                check_timeout()
                
                if not os.path.exists(model_path):
                    logger.error(f"Model file not found at {model_path}")
                    return None
                    
                current_hash = self.get_model_hash(model_path)
                logger.info(f"Current model hash: {current_hash}")
                check_timeout()

                # Check if model needs to be reloaded
                if not force and self.model is not None and self.model_hash == current_hash:
                    self.last_used = time.time()
                    logger.info("Using cached model")
                    return self.model

                # Unload existing model if any
                self.unload_model()
                check_timeout()

                try:
                    # Step 1: Create model instance
                    logger.info("Step 1: Creating new model instance...")
                    # Try to create model in offline mode first
                    os.environ['TORCH_HOME'] = os.path.join(os.getcwd(), 'model', 'torch_home')
                    os.environ['TORCH_OFFLINE'] = '1'
                    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'  # Enable Metal fallback on macOS
                    
                    model = DeepfakeDetector(use_cuda=False)  # Force CPU mode for stability
                    logger.info("Model instance created successfully")
                    check_timeout()
                    
                    # Step 2: Load state dict
                    logger.info("Step 2: Loading state dict...")
                    try:
                        state_dict = torch.load(model_path, map_location='cpu')
                    except Exception as e:
                        logger.error(f"Error loading state dict: {str(e)}")
                        raise
                    logger.info("State dict loaded successfully")
                    check_timeout()
                    
                    # Step 3: Apply state dict
                    logger.info("Step 3: Applying state dict to model...")
                    try:
                        model.load_state_dict(state_dict)
                    except Exception as e:
                        logger.error(f"Error applying state dict: {str(e)}")
                        raise
                    logger.info("State dict applied successfully")
                    check_timeout()
                    
                    # Step 4: Set eval mode
                    logger.info("Step 4: Setting model to eval mode...")
                    model.eval()
                    logger.info("Model set to eval mode")
                    check_timeout()
                    
                    # Step 5: Test with dummy input
                    logger.info("Step 5: Testing model with dummy input...")
                    dummy_input = torch.randn(1, 3, 224, 224)
                    with torch.no_grad():
                        _ = model(dummy_input)
                    logger.info("Model test successful")
                    check_timeout()

                    # Step 6: Update model state
                    self.model = model
                    self.model_hash = current_hash
                    self.last_load_time = time.time()
                    self.last_used = time.time()
                    
                    memory_usage = memory_monitor.get_memory_usage()
                    logger.info(f"Model loaded successfully (Version: {current_hash[:8]})")
                    logger.info(f"Current memory usage: {memory_usage['rss_mb']:.2f}MB RSS")
                    logger.info(f"Total load time: {time.time() - start_time:.2f}s")
                    
                    return model
                    
                except TimeoutError:
                    logger.error("Model loading timed out")
                    if hasattr(self, 'model') and self.model is not None:
                        self.unload_model()
                    return None
                except Exception as e:
                    logger.error(f"Error during model loading: {str(e)}", exc_info=True)
                    if hasattr(self, 'model') and self.model is not None:
                        self.unload_model()
                    return None
                    
            except Exception as e:
                logger.error(f"Error in load_model: {str(e)}", exc_info=True)
                return None

    def get_model(self):
        """Get the current model, loading it if necessary"""
        if self.model is None:
            return self.load_model(force=True)
        return self.model

model_manager = ModelManager()

# Try to import TensorFlow components
try:
    from predict import predict_image as predict_tf, predict_video
    TF_AVAILABLE = True
except ImportError:
    print("TensorFlow not available")
    TF_AVAILABLE = False

if not (TORCH_AVAILABLE or TF_AVAILABLE):
    raise ImportError("Neither PyTorch nor TensorFlow models could be loaded")

UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'mp4', 'avi', 'mov'}

# Configure flask app
app = Flask(__name__, template_folder='templates')
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size

# Load the frontend HTML from templates folder
@app.route('/')
def home():
    try:
        # Try to render the template from the templates folder
        return render_template('html_frontend.html')
    except Exception as e:
        logger.error(f"Error rendering template: {str(e)}", exc_info=True)
        return jsonify({
            'success': False,
            'error': 'Error loading application interface',
            'details': str(e)
        }), 500

@app.route('/api/health')
def health():
    # Simple health check for frontend
    try:
        # Try getting the model
        model = model_manager.get_model()
        if model is None:
            logger.error("Could not load model in health check")
            return jsonify({'model_loaded': False})

        # Try a dummy prediction to check model
        test_img = None
        for f in os.listdir(app.config['UPLOAD_FOLDER']):
            if f.lower().endswith(('jpg', 'jpeg', 'png')):
                test_img = os.path.join(app.config['UPLOAD_FOLDER'], f)
                break

        if test_img and TORCH_AVAILABLE:
            logger.info(f"Running test prediction on {test_img}")
            result = model.predict(test_img)
            model_loaded = result['success']
        else:
            model_loaded = model is not None  # Consider model loaded if we got it from manager

        return jsonify({'model_loaded': model_loaded})
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return jsonify({'model_loaded': False})

# Utility functions
def cleanup_uploads():
    """Clean up files in the uploads directory"""
    try:
        for filename in os.listdir(UPLOAD_FOLDER):
            filepath = os.path.join(UPLOAD_FOLDER, filename)
            if os.path.isfile(filepath):
                os.remove(filepath)
    except Exception as e:
        logger.error(f"Error cleaning up uploads: {str(e)}", exc_info=True)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/detect', methods=['POST'])
@rate_limit
def detect_file():
    try:
        logger.info(f"Received file upload request from {request.remote_addr}")
        
        # Clean up any previous files
        cleanup_uploads()
        
        if 'file' not in request.files:
            logger.error("No file found in request")
            return jsonify({'success': False, 'error': 'No file uploaded'})
        
        file = request.files['file']
        if file.filename == '':
            logger.error("Empty filename received")
            return jsonify({'success': False, 'error': 'No file selected'})
            
        if not allowed_file(file.filename):
            logger.error(f"File type not allowed: {file.filename}")
            return jsonify({
                'success': False,
                'error': f'File type not allowed. Allowed types: {", ".join(ALLOWED_EXTENSIONS)}'
            })
        
        # Create a unique filename to avoid conflicts
        filename = secure_filename(f"{int(time.time())}_{file.filename}")
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        
        # Save the file
        try:
            file.save(filepath)
            logger.info(f"File saved successfully: {filepath}")
        except Exception as e:
            logger.error(f"Error saving file: {str(e)}", exc_info=True)
            return jsonify({'success': False, 'error': 'Error saving uploaded file'})
        
        try:
            model = model_manager.get_model()
            if model is None:
                raise RuntimeError("Could not load model")
            
            result = model.predict(filepath)
            
            # Clean up the uploaded file immediately after prediction
            try:
                os.remove(filepath)
                logger.info(f"Cleaned up file: {filepath}")
            except Exception as e:
                logger.warning(f"Could not remove uploaded file: {str(e)}")
            
            return jsonify(result)
            
        except Exception as e:
            logger.error(f"Error during prediction: {str(e)}", exc_info=True)
            # Clean up file on error
            try:
                os.remove(filepath)
            except:
                pass
            return jsonify({
                'success': False,
                'error': 'Error processing image',
                'details': str(e)
            })
            
    except Exception as e:
        logger.error(f"Unhandled error in detect_file: {str(e)}", exc_info=True)
        return jsonify({
            'success': False,
            'error': 'Internal server error',
            'details': str(e)
        }), 500

